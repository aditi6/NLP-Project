{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditi6/NLP-Project/blob/main/llama_convo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57V5PvsxeiY6"
      },
      "outputs": [],
      "source": [
        "!pip install llamaapi -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llamaapi import LlamaAPI\n",
        "import json\n",
        "# please change the api to your own key\n",
        "llama = LlamaAPI('LL-zUoQjcGaE0WvMKj0Wck7ZdocLqQPMCccjgodbEwUAQnf1GJku1T8Tav6w9wYGjPN')"
      ],
      "metadata": {
        "id": "TLh3GhvNe_sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llamaapi import LlamaAPI\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Initialize the Llama API with your API token\n",
        "# llama = LlamaAPI('your_llama_api_token')\n",
        "\n",
        "def get_response_from_llama(prompt):\n",
        "    # Build the API request JSON with the user's prompt\n",
        "    api_request_json = {\n",
        "        \"model\": \"llama-13b-chat\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Execute the API request\n",
        "    response = llama.run(api_request_json)\n",
        "\n",
        "    # Parse and print the JSON response\n",
        "    if response.ok:\n",
        "        return json.dumps(response.json(), indent=2)\n",
        "    else:\n",
        "        return \"Error in Llama API response\"\n",
        "\n",
        "def main():\n",
        "    # Load the prompts from the CSV file\n",
        "    df = pd.read_csv('./generated_prompts.csv')\n",
        "\n",
        "    # Initialize a new column for the responses\n",
        "    df['response'] = ''\n",
        "\n",
        "    # Process each prompt and get responses from Llama\n",
        "    for index, row in df.iterrows():\n",
        "        prompt = f\"scenario:{row['scenario']}, character1:{row['character1']}, goal1:{row['goal1']}, character2:{row['character2']}, goal2:{row['goal2']}, task:{row['task']}\"\n",
        "        response = get_response_from_llama(prompt)\n",
        "        df.at[index, 'response'] = response\n",
        "\n",
        "    # Save the updated DataFrame back to the CSV\n",
        "    df.to_csv('./generated_response.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "RCQjfGhLfDKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama1 = LlamaAPI('LL-1JjExK2KMuL9qH30YOKdUvtk98gtZ0umxNmT8IOoWXweIqGiWmyBE8PI6InSMAUl')\n",
        "llama2 = LlamaAPI('LL-1JjExK2KMuL9qH30YOKdUvtk98gtZ0umxNmT8IOoWXweIqGiWmyBE8PI6InSMAUl')"
      ],
      "metadata": {
        "id": "5XdgsPDCYCC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llamaapi import LlamaAPI\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Initialize the Llama API with your API token\n",
        "# llama = LlamaAPI('your_llama_api_token')\n",
        "\n",
        "def get_response_from_llama(llama_instance, prompt, conversation_history=None):\n",
        "    # Convert conversation history to proper format (only string contents)\n",
        "\n",
        "\n",
        "    # Build the API request JSON with the formatted conversation history and new prompt\n",
        "\n",
        "    if conversation_history is not None:\n",
        "      conversation_history=list(conversation_history)\n",
        "      messages = [{\"role\": \"system\", \"content\": \"\"}]\n",
        "      for entry in conversation_history:\n",
        "        messages.append({\"role\": \"user\", \"content\": str(entry)})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": str(prompt)})\n",
        "\n",
        "    api_request_json = {\n",
        "        \"model\": \"llama-13b-chat\",\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": str(prompt)}]\n",
        "    }\n",
        "\n",
        "    # Execute the API request\n",
        "    response = llama_instance.run(api_request_json)\n",
        "\n",
        "    # Parse and print the JSON response\n",
        "    if response.ok:\n",
        "        return json.dumps(response.json(), indent=2)\n",
        "    else:\n",
        "        return \"Error in Llama API response\"\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load the prompts from the CSV file\n",
        "    df = pd.read_csv('./generated_prompts_demo.csv')\n",
        "\n",
        "    # Initialize a new column for the responses\n",
        "    df['response'] = ''\n",
        "\n",
        "    # Process each prompt and get responses from Llama\n",
        "    for index, row in df.iterrows():\n",
        "\n",
        "\n",
        "        Init_prompt_llama1 = f\"The scenario is:{row['scenario']}, You are to act as a particular character. The following is the description of your character:{row['character1']}, Your goals are as follows:{row['goal1']}. You will be interacting with another character. You need to negotiate and come to a conclusion in exactly 5 iterations excluding this one. Do not start generating the conversation until you are given the keyword START. You will be starting the conversation and you will be allowed to generate 5 responses including the starting in an alternate back-and-forth fashion with your opponent. You must come to a conclusion in the conversation by your 5th response.\"\n",
        "        Init_prompt_llama2 = f\"The scenario is:{row['scenario']}, You are to act as a particular character. The following is the description of your character:{row['character2']}, Your goals are as follows:{row['goal2']}. You will be interacting with another character. You need to negotiate and come to a conclusion in exactly 5 iterations excluding this one. Your opponent will be starting the conversation. Do not start generating your response until you recieve the first conversation statement from your opponent. After this you will be allowed to generate a maximum of 5 responses and you must come to a conclusion in the conversation by your 5th response.\"\n",
        "        init_response1 = get_response_from_llama(llama1,Init_prompt_llama1)\n",
        "        init_response2 = get_response_from_llama(llama2,Init_prompt_llama2)\n",
        "\n",
        "        conversation_history1=Init_prompt_llama1+\"\\n\"+init_response1\n",
        "        conversation_history2=Init_prompt_llama2+\"\\n\"+init_response2\n",
        "\n",
        "        llama1_response=get_response_from_llama(llama1,f\"START\",conversation_history1)\n",
        "        response=llama1_response\n",
        "        conversation_history1=conversation_history1+\"\\n\"+llama1_response\n",
        "\n",
        "\n",
        "\n",
        "        for i in range(4):\n",
        "          llama2_response=get_response_from_llama(llama2,llama1_response,conversation_history2)\n",
        "          response=response+\"\\n\"+llama2_response\n",
        "          conversation_history2=conversation_history2+\"\\n\"+llama1_response+\"\\n\"+llama2_response\n",
        "          llama1_response=get_response_from_llama(llama1,llama2_response,conversation_history1)\n",
        "          response=response+\"\\n\"+llama1_response\n",
        "          conversation_history1=conversation_history1+\"\\n\"+llama2_response+\"\\n\"+llama1_response\n",
        "\n",
        "\n",
        "        llama2_response=get_response_from_llama(llama2,llama2_response,conversation_history2)\n",
        "        response=response+\"\\n\"+llama2_response\n",
        "        conversation_history2=conversation_history2+\"\\n\"+llama1_response+\"\\n\"+llama2_response\n",
        "\n",
        "        df.at[index, 'response'] = response\n",
        "\n",
        "    # Save the updated DataFrame back to the CSV\n",
        "    df.to_csv('./generated_response.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Nm6aUIqwklyu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}